{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Analyzing Citibike Station Activity using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the database of snapshots of Citibike stations statuses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import MySQLdb as mdb\n",
    "import matplotlib \n",
    "matplotlib.style.use(['seaborn-talk', 'seaborn-ticks', 'seaborn-whitegrid'])\n",
    "matplotlib.rcParams['figure.figsize'] = (20,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first fetch the data from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = mdb.connect(host = 'localhost', \n",
    "                  user = 'root', \n",
    "                  passwd = 'dwdstudent2015', \n",
    "                  charset='utf8', \n",
    "                  use_unicode=True, \n",
    "                  database='citibike');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to retrieve all the data, we will see that we have way too many data points (more than 10 million). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = con.cursor(mdb.cursors.DictCursor)\n",
    "cur.execute(\"SELECT COUNT(*) AS cnt FROM citibike.Docks_Status\")\n",
    "result = list(cur.fetchall())\n",
    "cur.close()\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving millions of data points from the database is going to take long time, and may cause memory errors. \n",
    "\n",
    "#### Pushing part of the computation down to the database\n",
    "\n",
    "The goal of our analysis is to see how bike usage varies over time. Therefore, we can reduce the amount of retrieved data by asking to get back only averages over a period of, say, 60 minutes. \n",
    "\n",
    "Unfortunately, SQL does not provide elegant tools for handling time series, do we are going to resort to a few \"hacks\". We are going to round the `last_communication_time` field in the database into periods of 15 minutes (i.e., 900 seconds), and then compute the average level of \"fullness\" of the bike station (defined as number of bikes over the number of docks in the station).\n",
    "\n",
    "* The command `DATE_FORMAT(last_communication_time, '%Y-%m-%d %H:00:00')` truncates each timestamp to the nearest hour.\n",
    "* We also limit our query to only data from February 13 to March 13th.\n",
    "* We also limit our query only to statuses where the station was operating and reported back a proper status\n",
    "* We GROUP BY timestamp and station, and we compute the average fullness level of the station over that time.\n",
    "\n",
    " *(Note: The DATE_FORMAT approach works for truncating the timestamp The following, more complicated, code can work for arbitrary time periods. For example, to get 900 intervals (ie 15 mins), we can do `CONCAT(DATE(last_communication_time), ' ',  SEC_TO_TIME((TIME_TO_SEC(last_communication_time) DIV 900) * 900))`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT * \n",
    "FROM citibike.stations\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = con.cursor()\n",
    "cur.execute(query)\n",
    "df = pd.DataFrame(list(cur.fetchall()), columns=['id', 'bikes', 'timestamp'])\n",
    "cur.close()\n",
    "# We retrieved the data in memory, so we do not need the database connection anymore.\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we reduced now our dataset from more than 10+ million data points to around half a million. That will give us a big speedup in our subsequent operations and can easily be handled in-memory by Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert into proper data types. \n",
    "\n",
    "*Note: We use the \"astype\" as opposed to \"pd.to_numeric\" for converting the bikes column, because the bikes variable\n",
    "that comes back from MySQL is a Decimal data type, and Pandas.to_numeric seems to have  problems converting Decimal data types. We can use the technique from http://stackoverflow.com/questions/7483363/python-mysqldb-returns-datetime-date-and-decimal if we want to get back floats instead of Decimals from MySQL.*\n",
    "\n",
    "*Note2: We use the 'downcast' option to reduce the size of the variables. This reduces memory needs, and can (slightly) improve execution time.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['bikes'] = df['bikes'].astype(float)\n",
    "df['bikes'] = pd.to_numeric(df['bikes'], downcast='float')\n",
    "df['id'] = pd.to_numeric(df['id'], downcast='unsigned')\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Analysis\n",
    "\n",
    "As a first step, let's see how the status of the bike stations evolves over time. We compute the average \"fullness\" of all the bike stations over time. We can use the `groupby` function of pandas, and compute the `mean()` for the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that this also returns an average for the station ID's which is kind of useless\n",
    "# We will eliminate these next.\n",
    "df.groupby('timestamp').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('timestamp').mean()['bikes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the activity over time. We can see that the percentage of bikes in the stations falls from 35% overnight to 30% during the morning and evening commute times, while the average availability during the day is around 31%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('timestamp').mean()['bikes'].plot(\n",
    "    figsize=(20,10), grid=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do also the seasonal decomposition to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo pip3 install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "time_series = df.groupby('timestamp').mean()['bikes']\n",
    "\n",
    "# We decompose assumming a 24-hour periodicity. There is a weekly component as well, which we ignore\n",
    "decomposition = seasonal_decompose(time_series, freq=168)\n",
    "\n",
    "seasonal = decomposition.plot()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining Time Series per Station\n",
    "\n",
    "We now create a pivot table, to examine the time series for individual stations.\n",
    "\n",
    "Notice that we use the `fillna` method, where we fill the cells where we do not have values using the prior, non-missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df2 = df.pivot_table(\n",
    "    index='timestamp', \n",
    "    columns='id', \n",
    "    values='bikes', \n",
    "    aggfunc=np.mean\n",
    ").interpolate(method='time') \n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the time series for *all* bike stations, for a couple of days in February."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.plot(\n",
    "    alpha=0.05, \n",
    "    color='b', \n",
    "    legend=False, \n",
    "    figsize=(20,10), \n",
    "    xlim=('2017-02-15','2017-02-17')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's limit our plot to just two stations:\n",
    "* Station 3260 at \"Mercer St & Bleecker St\"\n",
    "* Station 161 at \"LaGuardia Pl & W 3 St\"\n",
    "\n",
    "which are nearby and tend to exhibit similar behavior. Remember that the list of stations is [available as a JSON](https://feeds.citibikenyc.com/stations/stations.json) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[[161, 3260, 260]].plot(\n",
    "    alpha=0.5,  \n",
    "    legend=False, \n",
    "    figsize=(20,10), \n",
    "   xlim=('2017-02-15','2017-02-27')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Bike Stations with Similar Behavior\n",
    "\n",
    "For our next analysis, we are going to try to find bike stations that have similar behaviors over time. A very simple technique that we can use to find similar time series is to treat the time series as vectors, and compute their correlation. Pandas provides the `corr` function that can be used to calculate the correlation of columns. (If we want to compute the correlation of rows, we can just take the transpose of the dataframe using the `transpose()` function, and compute the correlations there.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = df2.corr(method='pearson')\n",
    "similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the similarities of the two stations that we examined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = [161, 3260]\n",
    "\n",
    "similarities[stations].loc[stations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For bookkeeping purposes, we are going to drop columns that contain NaN values, as we cannot use such similarity values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities.dropna(axis=0, how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to convert our similarities into distance metrics, that are positive, and bounded to be between 0 and 1.\n",
    "\n",
    "* If two stations have correlation 1, they behave identically, and therefore have distance 0, \n",
    "* If two stations have correlation -1, they have exactly the oppositite behaviors, and therefore we want to have distance 1 (the max) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = ((.5*(1-similarities))**2)\n",
    "distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Based on Distances\n",
    "\n",
    "Without explaining too much about clustering, we are going to use a clustering technique and cluster together bike stations that are \"nearby\" according to our similarity analysis. The code is very simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "cluster = KMeans(n_clusters=2)\n",
    "cluster.fit(distances.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now take the results of the clustering and associate each of the data points into a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.DataFrame(list(zip(distances.index.values.tolist(), cluster.labels_)), columns = [\"id\", \"cluster\"])\n",
    "labels.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many stations in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.groupby('cluster').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Time Series Clusters\n",
    "\n",
    "We will start by assining a color to each cluster, so that we can plot each station-timeline with the cluster color. (We put a long list of colors, so that we can play with the number of clusters in the earlier code, and still get nicely colored results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = list(['r','b', 'g', 'm', 'y', 'k', 'w', 'c'])\n",
    "labels['color'] = labels['cluster'].apply(lambda cluster_id : colors[cluster_id]) \n",
    "labels.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_plot = df2.plot(\n",
    "    alpha=0.02, \n",
    "    legend=False, \n",
    "    figsize=(20,5), \n",
    "    color=labels[\"color\"],\n",
    "    #xlim=('2017-02-15','2017-02-17')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot still looks messy. Let's try to plot instead a single line for each cluster. To represent the cluster, we are going to use the _median_ fullness value across all stations that belong to a cluster, for each timestamp. For that, we can again use a pivot table: we define the `timestamp` as one dimension of the table, and `cluster` as the other dimension, and we use the `percentile` function to compute the median. \n",
    "\n",
    "For that, we first _join_ our original dataframe with the results of the clustering, using the `merge` command, and add an extra column that includes the clusterid for each station. Then we compute the pivot table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "median_cluster = df.merge(\n",
    "    labels, \n",
    "    how='inner', \n",
    "    on='id'\n",
    ").pivot_table(\n",
    "    index='timestamp', \n",
    "    columns='cluster', \n",
    "    values='bikes', \n",
    "    aggfunc=lambda x: np.percentile(x, 50) # median\n",
    ")\n",
    "\n",
    "median_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can plot the medians for the two clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_plot = median_cluster.plot(\n",
    "        figsize=(20,5), \n",
    "        linewidth = 2, \n",
    "        alpha = 0.75,\n",
    "        color=colors,\n",
    "        ylim = (0,0.75),\n",
    "        grid = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just for fun and for visual decoration, let's put the two plots together. We are going to fade a lot the individual station time series (by putting the `alpha=0.01`) and we are going to make more prominent the median lines by increasing their linewidths. We will limit our plot to one week's worth of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_plot = df2.plot(\n",
    "    alpha=0.01, \n",
    "    legend=False, \n",
    "    figsize=(20,5), \n",
    "    color=labels[\"color\"]\n",
    ")\n",
    "\n",
    "median_cluster.plot(\n",
    "    figsize=(20,5), \n",
    "    linewidth = 3, \n",
    "    alpha = 0.5,\n",
    "    color=colors, \n",
    "    xlim=('2017-02-13','2017-02-20'),\n",
    "    ax = stations_plot\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
