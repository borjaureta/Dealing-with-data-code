{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Introduction to Time Series and Forecasting: Time Series Models\n",
    "\n",
    "Based on the book [Introduction to Time Series and Forecasting](http://www.masys.url.tw/Download/2002-Brockwell-Introduction%20Time%20Series%20and%20Forecasting.pdf) by Brockwell and Davis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A common objective of any time series analysis is to \"understand\" the data. While the exact semantics of the term \"understand\" will vary, a common objective of all the analysis will be to create a *model* that describes the data.\n",
    "\n",
    "#### What is a time series model?\n",
    "\n",
    "Notice that the data that we have, no matter how big, do not tell us anything about the values of the time series before, after, or even between the data points that we have observed and recorded. For performing such tasks, we need a model. A time series model is a \"compact\" way to represent the time series, and the model often allows us to make  predictions or inferences about the parts of the time series that we have not observed.\n",
    "\n",
    "We may, for example, be able to represent the accidental deaths data and in the data for Lake Huron, as the sum of a speciﬁed trend, and seasonal and random terms. For the interpretation of many data series, it is important to recognize the presence of seasonal components and to remove them so as not to confuse them with long-term trends. This process is known as seasonal adjustment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.style.use('seaborn-poster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Some Simple Time Series Models\n",
    "\n",
    "#### Defining a Time Series Model\n",
    "\n",
    "* Data: We have a series of data points (the \"realization\" of the time series): $x_1, x_2, \\ldots, x_t, \\ldots$\n",
    "* Model Random Variables: We assume that each data point $x_t$ is an instantiation of a random variable $X_t$. \n",
    "\n",
    "Most time series models deal with two main objectives:\n",
    "\n",
    "* Expected values: find the expected value $E(X_t)$ of $X_t$ for every value of $t$\n",
    "* Dependencies: find the expected value $E(X_{t+h} | X_t)$ of $X_{t+h}$ given the value of $X_t$ (also called  \"covariances\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Some Zero-Mean Models\n",
    "\n",
    "##### iid noise\n",
    "\n",
    "The simplest model for a time series has is one in which there is no trend or seasonal component, all variables has zero mean, and all observations are independent and identically distributed (iid). While iid noise is typically uninteresting process per se, it is and important component when building more complex models, as it is the components that typically models the \"inherent variability\" of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a time series of 50 random data points\n",
    "# from the “standard normal” distribution (mean zero, standard deviation one)\n",
    "# http://docs.scipy.org/doc/numpy/reference/generated/numpy.random.randn.html#numpy.random.randn\n",
    "ts = np.random.randn(50)\n",
    "# Plot the time series\n",
    "plt.title(\"Gaussian (0,1) IID noise\")\n",
    "plt.plot(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Random walk \n",
    "\n",
    "The random walk is obtained by cumulatively summing iid noise. For example, consider a stock: Assume that every day, the price of the stock changes according to random iid noise. Let's plot the outcome. (Try to run the code a few times and see the different outcomes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "upper_limit = 365\n",
    "# Create a time series of random data points\n",
    "iid_ts = np.random.randn(upper_limit)\n",
    "# Compute the cumulative sum\n",
    "random_walk_ts = np.cumsum(iid_ts)\n",
    "# Plot the time series\n",
    "plt.title(\"Random Walk with Gaussian(0,1) IID noise steps\")\n",
    "plt.xlim(0,upper_limit) \n",
    "plt.plot(random_walk_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### A more advanced random walk\n",
    "\n",
    "In stocks, the overall price of a stock is defined by a cumulative multiplication of daily return. Assuming that the daily returns are log-normal variables (i.e., their log is normally distributed), with a zero mean (i.e., long term the stock remains stable), and a daily variance of 1%, we can construct the following model for stock prices, evolving over a year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# The length of our time series\n",
    "ts_length = 365\n",
    "# The expected yearly return\n",
    "mean_yearly_return = 0.0\n",
    "# Converting the yearly return into a daily return\n",
    "mean_daily_return = (1+mean_yearly_return)**(1.0/ts_length)-1\n",
    "# Our expected stock variance each day.\n",
    "# We set it at a +/- 1% daily movement\n",
    "daily_variance = 0.01\n",
    "# Create a time series of random daily returns\n",
    "iid_ts = np.random.lognormal(mean = mean_daily_return, sigma=daily_variance, size=ts_length)\n",
    "# Compute the cumulative product\n",
    "random_walk_ts = np.cumprod(iid_ts)\n",
    "# Plot the time series\n",
    "plt.title(\"Random Walk with lognormal(0,1) IID noise steps\")\n",
    "plt.xlim(0,ts_length) \n",
    "plt.plot(random_walk_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Simulations: What could have been\n",
    "\n",
    "Here is not an example of why a model can be useful. When we deal with time series, we often see just one instantiation of the time series, describing what happened. However, we also want to have an understanding what _could_ have happened. For example, we would like to understand better how a stock would move, we could run multiple simulatations and see the expected results. For example, below we will create 1000 simulations of the random walk discussed above, and see the expected results of a stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# The length of our time series\n",
    "ts_length = 365\n",
    "# The expected yearly return\n",
    "mean_yearly_return = 0.0\n",
    "# Converting the yearly return into a daily return\n",
    "mean_daily_return = (1+mean_yearly_return)**(1.0/ts_length)-1\n",
    "# Our expected stock variance each day.\n",
    "# We set it at a +/- 1% daily movement\n",
    "daily_variance = 0.01\n",
    "\n",
    "# We will do 1000 simulations of the random walk\n",
    "plt.title(\"Monte Carlo Simulation of Random Walks with lognormal IID noise steps\")\n",
    "plt.xlim(0,upper_limit)\n",
    "for _ in range(1000):\n",
    "    # Create a time series of random daily returns\n",
    "    iid_ts = np.random.lognormal(mean = mean_daily_return, sigma=daily_variance, size=ts_length)\n",
    "    # Compute the cumulative product\n",
    "    random_walk_ts = np.cumprod(iid_ts)\n",
    "    # Add the line in the plot. \n",
    "    # We set the transparency of each line to be high (alpha=0.01 means 99% transparent) \n",
    "    plt.plot(random_walk_ts, alpha=0.01, color='b')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Plotting Quantiles\n",
    "\n",
    "And now we will take this Monte Carlo simulation, and we will compute the quantiles of the simulation. \n",
    "\n",
    "Specifically, we will examine the threshold for the 10% lowest performing stocks, the 25% lowest perfoming stocks, 50%, 75%, and 90%. For that we first create a dataframe, with multiple columns, one column per single simulation of the time series. Then, we run the [`quantile`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.quantile.html) command, computing the quantile numbers for each row of the dataframe (hence the use of `axis=1`). This returns a dataframe with 5 rows (one row per quantile), and 365 columns; to reshape it, we take the _transpose_ (hence the `.T` at the end) and we plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# The length of our time series\n",
    "ts_length = 365\n",
    "# The expected yearly return\n",
    "mean_yearly_return = 0.0\n",
    "# Converting the yearly return into a daily return\n",
    "mean_daily_return = (1+mean_yearly_return)**(1.0/ts_length)-1\n",
    "# Our expected stock variance each day.\n",
    "# We set it at a +/- 1% daily movement\n",
    "daily_variance = 0.01\n",
    "\n",
    "# We will do 1000 simulations of the random walk\n",
    "for x in range(1000):\n",
    "    # Create a time series of random daily returns\n",
    "    iid_ts = np.random.lognormal(mean = mean_daily_return, sigma=daily_variance, size=ts_length)\n",
    "    # Compute the cumulative product\n",
    "    random_walk_ts = np.cumprod(iid_ts)\n",
    "    # Add the line in the dataframe. \n",
    "    df.insert(x, \"Run\"+str(x), random_walk_ts)\n",
    "\n",
    "quantiles = df.quantile(q=[0.1,0.25,0.5,0.75,0.90], axis=1).T\n",
    "quantiles.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "#### Models with Trend and Seasonality\n",
    "\n",
    "In several of the time series examples shown earlier, there is a clear trend in the data. An increasing trend is apparent in both the Australian red wine sales and the population of the U.S.A. In both cases a zero-mean model for the data is clearly inappropriate. The graph of the population data, which contains no apparent periodic component, suggests trying a model of the form $X_t =  m_t +Y_t$ where $m_t$ is the a time series called the *trend component* and $Y_t$ is a time series with zero mean (see above). \n",
    "\n",
    "\n",
    "##### Extracting the trend component\n",
    "\n",
    "One technique that we can use to extract the trend component from a time series is by fitting a function to the time series, using for example the _least squares_ procedure. For example, for the US population date, we will try to fit a quadratic function of the form:\n",
    "\n",
    "$m_t = a_2 \\cdot t^2 + a_1 \\cdot t + a_0$\n",
    "\n",
    "To ﬁt a function of the to the population data we will first normalize the year data, and relabel the time axis so that $t=1$ corresponds to 1780 and $t=24$ corresponds to 2010. Then we will utilize the `statsmodel` package of Python to fit an ordinary least squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!sudo -H pip3 install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "\n",
    "df = pd.read_csv(\"data/us-population2.txt\", sep=' ', thousands=',')\n",
    "df[\"US_Population\"] = pd.to_numeric(df[\"US_Population\"])\n",
    "# Normalize/relabel the time variables\n",
    "df[\"Year_Norm\"] = (pd.to_numeric(df[\"Year\"])-1770)/10\n",
    "# Add a \"constant\" column, to be used later while we fit the trend function\n",
    "df[\"Constant\"] = [1]*len(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Fit a function using the ordinary least squares process (OLS)\n",
    "model = sm.ols(formula = 'US_Population ~ np.power(Year_Norm, 2) + Year_Norm + Constant', data = df).fit()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Plotting using Matplotlib directly\n",
    "# fig, ax = plt.subplots(figsize=(15,10))\n",
    "# ax.plot(df[\"Year\"], df[\"US_Population\"], 'o', label=\"data\")\n",
    "# ax.plot(df[\"Year\"], df[\"Prediction\"], 'r--.', label=\"prediction\")\n",
    "# ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Adding predictions in the dataframe, removing unecessary columns\n",
    "# setting Year as the index, and calculating residuals\n",
    "df[\"Prediction\"] = pd.to_numeric(model.fittedvalues).astype(int)\n",
    "df.set_index(keys=\"Year\", inplace=True)\n",
    "df.drop([\"Year_Norm\", \"Constant\"], axis=1, inplace=True)\n",
    "df[\"Residuals\"] = df[\"US_Population\"] - df[\"Prediction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Plotting the actual data and the prediction\n",
    "df[[\"US_Population\",\"Prediction\"]].plot(style=['bo','r--'], label=[\"data\", \"prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Plotting the residuals\n",
    "df[\"Residuals\"].plot(style='bo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will now repeat the process for the Lake Huron dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/lake-huron.txt\", sep='\\t')\n",
    "df[\"Year_Norm\"] = (pd.to_numeric(df[\"Year\"])-1874)\n",
    "# Add a \"constant\" column, to be used later while we fit the trend function\n",
    "df[\"Constant\"] = [1]*len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_quadratic = sm.ols(formula = 'Level ~ np.power(Year_Norm, 2) + Year_Norm + Constant', data = df).fit()\n",
    "model_linear = sm.ols(formula = 'Level ~ Year_Norm + Constant', data = df).fit()\n",
    "\n",
    "df[\"Prediction_Quadratic\"] = pd.to_numeric(model_quadratic.fittedvalues)\n",
    "df[\"Prediction_Linear\"] = pd.to_numeric(model_linear.fittedvalues)\n",
    "df.set_index(keys=\"Year\", inplace=True)\n",
    "df.drop([\"Year_Norm\", \"Constant\"], axis=1, inplace=True)\n",
    "df[\"Residuals_Quadratic\"] = df[\"Level\"] - df[\"Prediction_Quadratic\"]\n",
    "df[\"Residuals_Linear\"] = df[\"Level\"] - df[\"Prediction_Linear\"]\n",
    "df[[\"Level\",\"Prediction_Quadratic\", \"Prediction_Linear\"]].plot(style=['b-','r--', 'g--'], \n",
    "                                                               label=[\"data\", \"prediction (quadratic)\", \"prediction (linear)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's take a look at the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df[[\"Residuals_Quadratic\", \"Residuals_Linear\"]].plot(style=['ro-','go-'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Lag Plots and Autocorrelation\n",
    "\n",
    "One interesting thing about these residuals is that they are \"smooth\", and that the sign of the error at time $t$ seems to be predictive of the sign of the erorr at time $t+1$. Such dependency would indicate that there is a *autocorrelation* in the errors. We can use two types of plots to assess the existence of autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import lag_plot\n",
    "lag_plot(df[\"Residuals_Linear\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This lag plot indicates a clear linear dependency between two consecutive residual values, signaling an autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import autocorrelation_plot\n",
    "\n",
    "autocorrelation_plot(df[\"Residuals_Quadratic\"])\n",
    "autocorrelation_plot(df[\"Residuals_Linear\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "#### A General Approach to Time Series Modeling\n",
    "\n",
    "### Stationary Models and the Autocorrelation Function\n",
    "#### The Sample Autocorrelation Function\n",
    "#### A Model for the Lake Huron Data\n",
    "\n",
    "### Estimation and Elimination of Trend and Seasonal Components\n",
    "####  Estimation and Elimination of Trend in the Absence of Seasonality\n",
    "#### Estimation and Elimination of Both Trend and Seasonality\n",
    "\n",
    "### Testing the Estimated Noise Sequence\n",
    "\n",
    "### Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
