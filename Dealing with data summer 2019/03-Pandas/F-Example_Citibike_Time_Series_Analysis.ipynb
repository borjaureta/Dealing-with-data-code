{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Analyzing Citibike Station Activity using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the database of snapshots of Citibike stations statuses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import pandas as pd\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use(['seaborn-talk', 'seaborn-ticks', 'seaborn-whitegrid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo pip3 install -U sklearn geopandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first fetch the data from the database. The original SQL query on the raw data is the following, and is computing the average capacity of a bike station in hourly intervals.\n",
    "\n",
    "```sql\n",
    "    SELECT S.station_id AS id, \n",
    "           AVG(num_bikes_available) AS available_bikes,\n",
    "           AVG(num_docks_available) AS available_docks,\n",
    "           AVG(num_bikes_disabled) AS disabled_bikes,\n",
    "           AVG(num_docks_disabled) AS disabled_docks,\n",
    "           T.capacity, T.lat, T.lon, T.name,\n",
    "           DATE_FORMAT(last_reported, '%%Y-%%m-%%d %%H:00:00') AS communication_time \n",
    "    FROM citibike_new.Status S JOIN citibike_new.Stations T ON T.station_id = S.station_id\n",
    "    WHERE  is_renting=1 AND is_installed=1 AND T.capacity>0 AND last_reported>='2017-10-01' AND last_reported<='2017-11-15'\n",
    "    GROUP BY S.station_id, communication_time\n",
    "```\n",
    "\n",
    "For speed, we store the results of this query in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "conn_string = 'mysql://{user}:{password}@{host}/{database}?charset={encoding}'.format(\n",
    "    host = 'db.ipeirotis.org', \n",
    "    user = 'student',\n",
    "    password = 'dwdstudent2015',\n",
    "    database = 'citibike_fall2017',\n",
    "    encoding = 'utf8mb4')\n",
    "engine = create_engine(conn_string)\n",
    "\n",
    "query = '''\n",
    "SELECT *\n",
    "FROM status_fall2017\n",
    "'''\n",
    "df = pd.read_sql(query, con=engine)\n",
    "df.communication_time = pd.to_datetime(df.communication_time, format='%Y-%m-%d %H:%M:%S')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Analysis\n",
    "\n",
    "As a first step, let's see how the status of the bike stations evolves over time. We compute the average \"**fullness**\" of **all** the bike stations over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['percent_full'] = df.available_bikes / df.capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the activity over time. We can see that the percentage of bikes in the stations fluctuates over time, with around 10% difference from the daily-lows to the daily-highs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries = df.pivot_table(\n",
    "    index='communication_time', \n",
    "    values='percent_full',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "timeseries.plot( figsize=(15, 7), ylim=(0.25,0.55), xlim=('2017-10-01', '2017-10-08'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do also the seasonal decomposition to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "decomposition = seasonal_decompose(timeseries, model='multiplicative', freq=24)\n",
    "seasonal = decomposition.plot()  \n",
    "seasonal.set_size_inches(20, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining Time Series per Station\n",
    "\n",
    "We now create a pivot table, to examine the time series for individual stations.\n",
    "\n",
    "Notice that we use the `interpolate` method, where we fill the cells using interpolation using the non-missing values before and after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_timeseries = df.pivot_table(\n",
    "    index='communication_time', \n",
    "    columns='id', \n",
    "    values='percent_full', \n",
    "    aggfunc='mean'\n",
    ").interpolate(method='time') \n",
    "\n",
    "station_timeseries.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the time series for *all* bike stations, for a few days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_timeseries.plot(\n",
    "    alpha=0.1, \n",
    "    color='b', \n",
    "    legend=False, \n",
    "    figsize=(20,5), \n",
    "    xlim=('2017-10-01','2017-10-07'),\n",
    "    ylim=(0,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's limit our plot to just two stations:\n",
    "* Station 3260 at \"Mercer St & Bleecker St\"\n",
    "* Station 161 at \"LaGuardia Pl & W 3 St\"\n",
    "\n",
    "which are nearby and tend to exhibit similar behavior. Remember that the list of stations is [available as a JSON](https://feeds.citibikenyc.com/stations/stations.json) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df [ df.name.str.contains(\"Mercer\") & df.name.str.contains(\"Bleecker\") ]\n",
    "# df [ df.name.str.contains(\"LaGuardia\") ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_timeseries[ [161, 3260] ].plot(\n",
    "    alpha=0.5,  \n",
    "    legend=False, \n",
    "    figsize=(20,5), \n",
    "    xlim=('2017-10-11','2017-10-14'),\n",
    "    ylim=(0,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Bike Stations with Similar Behavior\n",
    "\n",
    "For our next analysis, we are going to try to find bike stations that have similar behaviors over time. A very simple technique that we can use to find similar time series is to treat the time series as vectors, and compute their correlation. Pandas provides the `corr` function that can be used to calculate the correlation of columns. (If we want to compute the correlation of rows, we can just take the transpose of the dataframe using the `transpose()` function, and compute the correlations there.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = station_timeseries.corr(method='pearson')\n",
    "similarities.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the similarities of the two stations that we examined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = [161, 3260]\n",
    "\n",
    "similarities[stations].loc[stations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 393: E 5 St & Avenue C\n",
    "# 2003: 1 Ave & E 18 St\n",
    "stations = [393, 2003] \n",
    "    \n",
    "similarities[stations].loc[stations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_timeseries[ [393, 2003]  ].plot(\n",
    "    alpha=0.5,  \n",
    "    legend=False, \n",
    "    figsize=(20,5), \n",
    "    xlim=('2017-10-01','2017-10-14'),\n",
    "    ylim=(0,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For bookkeeping purposes, we are going to drop stations that generate NaN values, as we cannot use such entries for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of stations with non-NaN similarity per station\n",
    "check = similarities.count()\n",
    "# Find the number of stations with less than the max number of similarities\n",
    "todrop = check[check < check.max()].index.values\n",
    "similarities.drop(todrop, axis='index', inplace=True)\n",
    "similarities.drop(todrop, axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Based on Distances\n",
    "\n",
    "Without explaining too much about clustering, we are going to use a clustering technique and cluster together bike stations that are \"nearby\" according to our similarity analysis. For this, we need to first convert our similarities to distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to convert our **similarities** into **distance** metrics. Our distance values will be always positive, and bounded between 0 and 1.\n",
    "\n",
    "* If two stations have correlation 1, they behave identically, and therefore have distance 0, \n",
    "* If two stations have correlation -1, they have exactly the oppositite behaviors, and therefore we want to have distance 1 (the max) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity goes from -1 to 1, so 1-similarity goes from 0 to 2.\n",
    "# So, we multiply with 0.5 to get it between 0 and 1, and then take the square\n",
    "distances = ((.5*(1-similarities))**2)\n",
    "distances.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering code is very simple: The code below will create two groups of stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "cluster = KMeans(n_clusters=2)\n",
    "cluster.fit(distances.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now take the results of the clustering and associate each of the data points into a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.DataFrame(list(zip(distances.index.values.tolist(), cluster.labels_)), columns = [\"id\", \"cluster\"])\n",
    "labels.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many stations in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.pivot_table(\n",
    "    index = 'cluster',\n",
    "    aggfunc = 'count'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Time Series Clusters\n",
    "\n",
    "We will start by assining a color to each cluster, so that we can plot each station-timeline with the cluster color. (We put a long list of colors, so that we can play with the number of clusters in the earlier code, and still get nicely colored results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = list(['red','blue', 'green', 'magenta', 'yellow', 'black', 'white', 'cyan'])\n",
    "labels['color'] = labels['cluster'].apply(lambda cluster_id : colors[cluster_id]) \n",
    "labels.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_plot = station_timeseries.plot(\n",
    "    alpha=0.1, \n",
    "    legend=False, \n",
    "    figsize=(20,5), \n",
    "    linewidth=1,\n",
    "    color=labels[\"color\"],\n",
    "    xlim=('2017-10-01','2017-10-07'),\n",
    "    ylim=(0,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot still looks messy. Let's try to plot instead a single line for each cluster. To represent the cluster, we are going to use the _median_ fullness value across all stations that belong to a cluster, for each timestamp. For that, we can again use a pivot table: we define the `communication_time` as one dimension of the table, and `cluster` as the other dimension, and we use the `median` function. \n",
    "\n",
    "For that, we first _join_ our original dataframe with the results of the clustering, using the `merge` command, and add an extra column that includes the clusterid for each station. Then we compute the pivot table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_cluster = df.merge(\n",
    "    labels, \n",
    "    how='inner', \n",
    "    on='id'\n",
    ").pivot_table(\n",
    "    index='communication_time', \n",
    "    columns='cluster', \n",
    "    values='percent_full', \n",
    "    aggfunc='median'\n",
    ")\n",
    "\n",
    "median_cluster.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can plot the medians for the two clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_cluster.plot(\n",
    "    figsize=(20,5), \n",
    "    linewidth = 2, \n",
    "    alpha = 0.75,\n",
    "    color=colors,\n",
    "    ylim = (0,1),\n",
    "    xlim=('2017-10-01','2017-10-10'),\n",
    "    grid = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just for fun and for visual decoration, let's put the two plots together. We are going to fade a lot the individual station time series (by putting the `alpha=0.005`) and we are going to make more prominent the median lines by increasing their linewidths. We will limit our plot to one week's worth of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_plot = station_timeseries.plot(\n",
    "    alpha=0.005, \n",
    "    legend=False, \n",
    "    figsize=(20,5), \n",
    "    color=labels[\"color\"]\n",
    ")\n",
    "\n",
    "median_cluster.plot(\n",
    "    figsize=(20,5), \n",
    "    linewidth = 3, \n",
    "    alpha = 0.5,\n",
    "    color=colors, \n",
    "    xlim=('2017-10-01','2017-10-14'),\n",
    "    ylim=(0,1),\n",
    "    ax = stations_plot\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will plot the stations in a map, to see where the clusters are located.\n",
    "\n",
    "Just to make things more interesting, we will also get information about the current status of the stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about current (real-time) status of the stations \n",
    "import requests\n",
    "\n",
    "url_status = 'https://gbfs.citibikenyc.com/gbfs/en/station_status.json'\n",
    "results = requests.get(url_status).json() \n",
    "status = results[\"data\"][\"stations\"]\n",
    "df_status = pd.DataFrame(status)\n",
    "df_status = df_status[ ['station_id', 'num_bikes_available'] ] #keep only station id and bikes available\n",
    "df_status.columns = ['id', 'bikes_available'] # rename the columns\n",
    "df_status.id = pd.to_numeric(df_status.id) # convert id to numeric\n",
    "df_status.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_status.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the necessary columns / rows\n",
    "map_df = df[ ['id', 'lon', 'lat', 'capacity'] ].drop_duplicates()\n",
    "# Join the list of stations with the cluster\n",
    "map_df = map_df.merge(\n",
    "    labels, \n",
    "    how='inner', \n",
    "    on='id'\n",
    ")\n",
    "# Join the list of stations with the current status\n",
    "map_df = map_df.merge(\n",
    "    df_status, \n",
    "    how='inner', \n",
    "    on='id'\n",
    ")\n",
    "map_df = map_df.set_index('id')\n",
    "# Reorder the columns\n",
    "map_df = map_df[ ['cluster', 'capacity', 'color', 'lon', 'lat', 'bikes_available'] ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the stations using a scatterplot with their lon/lat values\n",
    "# Use a different color for each cluster\n",
    "# Change the size of each dot according to the capacity\n",
    "map_df.plot.scatter(\n",
    "    x = 'lon',\n",
    "    y = 'lat',\n",
    "    edgecolors='Black',\n",
    "    c = map_df.color.values,\n",
    "    s = 3*map_df.bikes_available.values,\n",
    "    figsize=(10,10), alpha=0.6\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We draw a background with the map of the NYC neighborhoods\n",
    "import geopandas as gpd\n",
    "\n",
    "# Dataset from NYC Open Data: https://data.cityofnewyork.us/City-Government/Neighborhood-Tabulation-Areas/cpf4-rkhq\n",
    "shapefile = 'https://data.cityofnewyork.us/api/geospatial/cpf4-rkhq?method=export&format=Shapefile'\n",
    "df_nyc = gpd.GeoDataFrame.from_file(shapefile)\n",
    "# Plot the NYC neighborhoods map\n",
    "base = df_nyc.plot(\n",
    "    linewidth=0.5,\n",
    "    color='White',\n",
    "    edgecolor='Black',\n",
    "    figsize=(10, 10),\n",
    "    alpha=0.5)\n",
    "base.set_xlim((-74.10, -73.92))\n",
    "base.set_ylim((40.65, 40.82))\n",
    "\n",
    "map_df.plot.scatter(\n",
    "    x='lon',\n",
    "    y='lat',\n",
    "    edgecolors='Black',\n",
    "    c=map_df.color.values,\n",
    "    s=3 * map_df.bikes_available.values,\n",
    "    figsize=(10, 10),\n",
    "    alpha=0.6,\n",
    "    ax=base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping with Folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "map_center = [map_df.lat.median(), map_df.lon.median()]\n",
    "fmap = folium.Map(location=map_center, zoom_start=14,  tiles='cartodbpositron')\n",
    "fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_center = [map_df.lat.median(), map_df.lon.median()]\n",
    "fmap = folium.Map(location=map_center, zoom_start=13,  tiles='cartodbpositron')\n",
    "\n",
    "for name, row in map_df.iterrows():\n",
    "    \n",
    "    # Define the opacity of the marker to be proportional to the percentage of bikes in the station\n",
    "    # opacity = row[\"bikes_available\"]/row[\"capacity\"]\n",
    "    # Make the color green for the working stations, red otherwise\n",
    "    color = row[\"color\"]\n",
    "    # The size of the marker is proportional to the number of docks\n",
    "    size1 = row[\"capacity\"]/7+1\n",
    "    size2 = row[\"bikes_available\"]/7+1\n",
    "\n",
    "    \n",
    "    # We create a marker on the map with size proportional to its capacity\n",
    "    folium.CircleMarker(location=[row[\"lat\"], row[\"lon\"]], \n",
    "                        radius = size1,\n",
    "                        color='black', weight=1, \n",
    "                        fill=True,\n",
    "                        fill_opacity = 0.25,\n",
    "                        fill_color = color,\n",
    "                       ).add_to(fmap)\n",
    "    \n",
    "    # We create a marker with size proportional to the number of bikes \n",
    "    # This will be the \"internal\" circle, indicating how full the station is\n",
    "    # We make the opacity higher\n",
    "    folium.CircleMarker(location=[row[\"lat\"], row[\"lon\"]], \n",
    "                        radius = size2,\n",
    "                        color='black', weight=0.5, \n",
    "                        fill=True,\n",
    "                        fill_opacity = 0.75,\n",
    "                        fill_color=color,\n",
    "                       ).add_to(fmap)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the map as HTML (citibike.html). Also add the NYC neighborhoods \n",
    "\n",
    "# Dataset from NYC Open Data: https://data.cityofnewyork.us/City-Government/Neighborhood-Tabulation-Areas/cpf4-rkhq\n",
    "url = 'https://data.cityofnewyork.us/api/geospatial/cpf4-rkhq?method=export&format=GeoJSON'\n",
    "nyc_neighborhoods = gpd.GeoDataFrame.from_file(url)\n",
    "style_function = lambda x: {'fillOpacity': 0.0, 'weight': 0.5}\n",
    "folium.GeoJson(\n",
    "    nyc_neighborhoods,\n",
    "    name='geojson', \n",
    "    style_function=style_function\n",
    ").add_to(fmap)\n",
    "\n",
    "import os\n",
    "fn='citibike.html'\n",
    "tmpurl='file://{path}/{mapfile}'.format(path=os.getcwd(),mapfile=fn)\n",
    "fmap.save(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
