{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<center><u><h1>Apache Spark. Part 2</h1></u></center>\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame\n",
    "\n",
    "Apache Spark provides an additional data structure that is too similar to pandas's DataFrame (also called DataFrame) that supports SQL syntax and works much faster than RDD. It is represented in [Spark SQL](http://spark.apache.org/sql/) Spark's module designed for structured data processing. \n",
    "\n",
    "A DataFrame is like a table in a traditional relational database. But it has some internal optimizations. DataFrame can be constructed from a lot of sources like: structured tables, Hive tables, external databases, or from existing RDDs.\n",
    "\n",
    "Unlike the basic Spark RDD API, the interfaces of Spark SQL provide more information about the structure of both the data and the computation being performed. Internally, Spark SQL uses this extra information to perform extra optimizations. \n",
    "\n",
    "Let's create a simple `Dataframe`. First, we'll create a `DataFrame` from a list of tuples with names and ages of people. To create a DataFrame we use [`createDataFrame`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession.createDataFrame) method of SparkSession (by default it is `spark`, but it is possibility to create any other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list with People: (name, age)\n",
    "data = [\n",
    "    (\"John\",32),\n",
    "    (\"Alice\",23),\n",
    "    (\"Hannah\",19),\n",
    "    (\"Nick\", 27)\n",
    "]\n",
    "\n",
    "# Create a DataFrame from this list, we have to specify names of columns\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To display a DataFrame in table like form we may use the `show()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to create a DataFrame also from RDD. Let's create a RDD from our list of tuples. And then create a DataFrame from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make an RDD from a list of tuples `data` and print it using `collect` method\n",
    "rdd = sc.parallelize(data)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame from this RDD, specifying names of columns\n",
    "df = spark.createDataFrame(rdd, [\"Name\",\"Age\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the DataFrame `df` is created and you can use `show(n)` method to print the first `n` rows of the DataFrame (currently, the `df` consists of only 4 rows and we will skip `n` in brackets). By default `show()` return 20 rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the schema of DataFrame use `printSchema` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversely, you can convert a DataFrame back to RDD simply invoking it's `rdd` method. This will return a RDD of [`Row`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Row) objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert DataFrame to RDD\n",
    "dfrdd = df.rdd\n",
    "# Look at contents of RDD\n",
    "dfrdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a dataframe from RDD, vice versa \n",
    "df = spark.createDataFrame(dfrdd)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access data in row using attributes or indexing like:\n",
    "`row.Name` or `row[\"Name\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrieve first element\n",
    "row = dfrdd.collect()[0]\n",
    "print(row)\n",
    "# Access data by attribute\n",
    "print(row.Name)\n",
    "# Access data by indexing\n",
    "print(row[\"Age\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see performance difference between RDD and DataFrame. We will use the [Amazon Access Samples Data Set](https://archive.ics.uci.edu/ml/datasets/Amazon+Access+Samples). We take the small part of them. The used dataset contains 716063 rows. Open this file in your computer to see its structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read file with data\n",
    "amazon_rdd = sc.textFile(\"amazon_access_samples.csv\")\n",
    "# Find its header\n",
    "first = amazon_rdd.first()\n",
    "header = first.split(\",\")\n",
    "print(\"Column names: \", header)\n",
    "# Remove the first row and divide all next lines into separated columns\n",
    "amazon_rdd = amazon_rdd.filter(lambda x: x != first).map( lambda x: x.split(\",\") )\n",
    "amazon_df = spark.createDataFrame(amazon_rdd, header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amazon_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amazon_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will evaluate the time for ordering just created RDD `amazon_rdd` and DataFrame `amazon_df` by three columns `\"TARGET_NAME\"`, `\"LOGIN\"`, `\"REQUEST_DATE\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "t_df_start = time()  # Start time\n",
    "print(amazon_df.orderBy(\"TARGET_NAME\", \"LOGIN\", \"REQUEST_DATE\").take(5))\n",
    "t_df_end = time() - t_df_start  # Elapsed time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_rdd_start = time()\n",
    "print(amazon_rdd.sortBy(lambda a: (a[1], a[2], a[3])).take(5))\n",
    "t_rdd_end = time() - t_rdd_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a simple barchart using [matplotlib](http://matplotlib.org/) Python library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "objects = (\"DataFrame\", \"RDD\")   # bars' names\n",
    "x_pos = np.arange(len(objects))  # positions along X axis\n",
    "performance = [t_df_end, t_rdd_end]  # height of bars \n",
    "bar_width = 0.5\n",
    "plt.bar(x_pos, performance, bar_width, align='center', alpha=0.5)\n",
    "plt.xticks(x_pos, objects)\n",
    "plt.ylabel('Time')\n",
    "plt.title('Performance difference between DataFrame and RDD')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As you can see DataFrame works much faster at aggregation, particularly. By the way, DataFrames require much less memory at cashing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations with DataFrame\n",
    "DataFrame have many methods full list you can see [here](http://spark.apache.org/docs/2.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame).\n",
    "\n",
    "In the below table you see most common used of them:\n",
    "\n",
    "|Name|Description|\n",
    "|:---|------------|\n",
    "|`agg(*exprs)`|Aggregate on the entire DataFrame without groups.|\n",
    "|`collect()`|Returns all the records as a list of Row.|\n",
    "|`columns`|Returns all column names as a list.|\n",
    "|`count()`|Returns the number of rows in this DataFrame.|\n",
    "|`distinct()`|Returns a new DataFrame containing the distinct rows in this DataFrame.|\n",
    "|`drop(col)`|Returns a new DataFrame that drops the specified column.|\n",
    "|`filter(condition)`|Filters rows using the given condition.|\n",
    "|`first()`|Returns the first row as a Row.|\n",
    "|`foreach(f)`|Applies the `f` function to all Row of this DataFrame.|\n",
    "|`groupBy(*cols)`|Groups the DataFrame using the specified columns, so we can run aggregation on them.|\n",
    "|`orderBy(*cols, ascending=True)`|Returns a new DataFrame sorted by the specified column(s)|\n",
    "|`select(*cols)`|Compute the sum for each numeric columns for each group.|\n",
    "|`show(n=20)`|Prints the first n rows to the console.|\n",
    "|`take(num)`|Returns the first num rows as a list of Row.|\n",
    "|`toPandas()`|Returns the contents of this DataFrame as Pandas pandas.DataFrame.|\n",
    "|`withColumn(colName, col)`|Returns a new DataFrame by adding a column or replacing the existing column that has the same name.|\n",
    "|`write`|Interface for saving the content of the non-streaming DataFrame out into external storage.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we provide few examples of using the above listed commands.\n",
    "\n",
    "Let's display the first row of only one column `\"LOGIN\"` in the DataFrame `amazon_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amazon_df.select(\"LOGIN\").first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can select not only one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amazon_df.select(\"REQUEST_DATE\", \"AUTHORIZATION_DATE\", \"LOGIN\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To display all DataFrame column names use the `columns` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amazon_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select only unique records in the `\"ACTION\"` column using `distinct()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amazon_df.select(\"ACTION\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can delete some column if you don't need it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amazon_df.drop(\"ACTION\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`orderBy()` method allows ordering rows by values in one or many columns. Let's order the table at first by `\"TARGET_NAME\"`, then by `\"LOGIN\"` and after this by `\"REQUEST_DATE\"` in descendging order.\n",
    "\n",
    "We use also `toPandas()` for printing table in more pretty view as pandas' DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In this time we sort in descending order\n",
    "amazon_df.orderBy(\"TARGET_NAME\", \"LOGIN\", \"REQUEST_DATE\", ascending=False).toPandas().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay attention that previously removed column `\"ACTION\"` whatever exists in the `amazon_df` DataFrame. It is due to we simply remove it from the current output at calling `show()` command but not in the DataFrame in general, i.e. create a copy of this DataFrame. To delete any column in the DataFrame we shoul reasing it\n",
    "\n",
    "    amazon_df = amazon_df.drop(\"ACTION\")\n",
    "\n",
    "You can also group rows by one or few columns and apply the preferable aggregation to each group. For example, let's count how many rows are for each pair of `\"LOGIN\"` and `\"TARGET_NAME\"`. The obtained field (result of aggregation will get the name of aggregation operation) can be used in other method at once (below we use it for ordering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amazon_df.groupBy(\"LOGIN\", \"TARGET_NAME\").count().orderBy(\"count\", ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to do some operation with columns? For example convert data type of `\"REQUEST_DATE\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First we need to see what type of data already in DataFrame\n",
    "# Pay attention on the type of \"REQUEST_DATE\" column\n",
    "amazon_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert data type we will use the method `withColumn()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import datetime Spark's type\n",
    "from pyspark.sql.types import TimestampType\n",
    "# Spark works with its own function formats, so we should convert any our function to this format using `udf`\n",
    "# We should also define the data type of outputs of this function. In our case it is `TimestampType`\n",
    "from pyspark.sql.functions import udf\n",
    "from datetime import datetime\n",
    "\n",
    "def convert_dttm(x):\n",
    "    try:\n",
    "        # `datetime.strptime(date_string, format)` returns a datetime corresponding to date_string, parsed according to format. \n",
    "        # This \"REQUEST_DATE\"'s value 2010-04-20 09:33:25 has the format '%Y-%m-%d %H:%M:%S'\n",
    "        # The meaning of letter you may find here https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\n",
    "        return datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n",
    "    except:\n",
    "        return None\n",
    "# Convert Python function to Spark function\n",
    "convert_dttm_udf = udf(convert_dttm, TimestampType())\n",
    "\n",
    "amazon_df = amazon_df.withColumn(\"REQUEST_DATE\", convert_dttm_udf(amazon_df.REQUEST_DATE))\n",
    "# Check whether data type of \"REQUEST_DATE\" changed\n",
    "amazon_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can work with this column data as with datetime and extract the day from the date using `day` attribute of `datetime.datetime` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "get_day = udf(lambda x: x.day, IntegerType())\n",
    "amazon_df.withColumn(\"REQUEST_DAY\", get_day(amazon_df[\"REQUEST_DATE\"])).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay attention how we cut the displaying table using `limit()` method.\n",
    "\n",
    "Let's find maximum and minimum values of `\"LOGIN\"` column using `agg(params)` method. We can define `params` as a dictionary of the structure\n",
    "\n",
    "    { \"column name 1\": \"aggregation function 1\", \"column name 2\": \"aggregation function 2\", ... }\n",
    "   \n",
    "The full list of aggregation functions you may find [here](http://spark.apache.org/docs/2.0.0/api/python/pyspark.sql.html#module-pyspark.sql.functions). Particualrly, \n",
    "\n",
    "* `\"min\"` returns the minimal value\n",
    "* `\"avg\"` calculates the average value\n",
    "* `\"count\"` returns amount of records. etc.\n",
    "\n",
    "Let's find the maximal value in `\"LOGIN\"` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amazon_df.agg({\"LOGIN\": \"max\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find how much rows is unique using `count()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amazon_df.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data you already know how, now try to write it. Let's delete some column and write the data to csv. More about write you can find here http://spark.apache.org/docs/2.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amazon_df_write = amazon_df.drop(\"AUTHORIZATION_DATE\")\n",
    "amazon_df_write.write.csv(\"my_first_csv_file.csv\", mode=\"overwrite\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mode=\"overwrite\"` means that you will clear all content of the `\"my_first_csv_file.csv\"` and write the new content. To add new content to end of a file use `mode=\"append\"`.\n",
    "\n",
    "But when you open the folder you will see something like that:\n",
    "![alt text](images/1.png)\n",
    "And how to read this file? For this we need [**HiveContext**](http://spark.apache.org/docs/2.0.0/api/python/pyspark.sql.html#pyspark.sql.HiveContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext\n",
    "\n",
    "sqlContext = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you whant to set you own schema you can do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"ACTION\", StringType()),\n",
    "    StructField(\"TARGET_NAME\", StringType()),\n",
    "    StructField(\"LOGIN\", StringType()),\n",
    "    StructField(\"REQUEST_DATE\", TimestampType()),    \n",
    "])\n",
    "\n",
    "amazon_df_read = sqlContext.read.csv(\"my_first_csv_file.csv\", header=True, schema=schema)\n",
    "amazon_df_read.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amazon_df_read.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL in Spark\n",
    "Spark support SQL queries. Let's look how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First you need to register DataFrame as Table setting the name of using DataFrame (`amazon_df` in our case)\n",
    "# and the respective table name (`\"amazon_table\"` in our case)\n",
    "sqlContext.registerDataFrameAsTable(amazon_df, \"amazon_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write the query that selects all rows with `\"TARGET_NAME\"` less than `9521` but larger than `7315`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM amazon_table\n",
    "    WHERE TARGET_NAME BETWEEN '7315' AND '9521'\n",
    "    ORDER BY REQUEST_DATE DESC\n",
    "\"\"\"\n",
    "target_df = sqlContext.sql(query)\n",
    "target_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such filtering can be also done with the help of `filter()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amazon_df.filter( (amazon_df[\"TARGET_NAME\"] > '7315') & (amazon_df[\"TARGET_NAME\"] < '9521') ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, if you need to write few filter conditions like above you shoul wtire them in brackets and join using `&` (logical `AND`) sign if you need that both conditions fulfillment or `|` (logical `OR`) if any of them. If you need only one condition, for example, `amazon_df[\"TARGET_NAME\"] > '7315'` simply write\n",
    "\n",
    "    amazon_df.filter(amazon_df[\"TARGET_NAME\"] > '7315')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Classification with Apache Spark\n",
    "\n",
    "In this part of the lesson you will learn how to use machine learning algorithms provided by Apache Spark (we will perform only [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) as an example) to solve classfication problems. \n",
    "\n",
    "Let's remember that classfication is the problem of identifying to which of a set of categories a new observation belongs, on the basis of a training set of data containing observations whose category membership is known. Another words it is the process of organizing data into categories for its most effective and efficient use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnosis prediction\n",
    "\n",
    "\n",
    "In this example we will use the Wisconsin Diagnostic Breast Cancer (WDBC) dataset which categorizes breast tumor cases as either benign or malignant based on 9 features to predict the diagnosis. You need to save [this file](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data) from [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.html) into the folder where the current IPython notebook lies.\n",
    "\n",
    "For each cancer observation, we have the following information:\n",
    "\n",
    "1. Sample code number: id number \n",
    "2. Clump Thickness: 1 - 10 \n",
    "3. Uniformity of Cell Size: 1 - 10 \n",
    "4. Uniformity of Cell Shape: 1 - 10 \n",
    "5. Marginal Adhesion: 1 - 10 \n",
    "6. Single Epithelial Cell Size: 1 - 10 \n",
    "7. Bare Nuclei: 1 - 10 \n",
    "8. Bland Chromatin: 1 - 10 \n",
    "9. Normal Nucleoli: 1 - 10 \n",
    "10. Mitoses: 1 - 10 \n",
    "11. Class: (2 for benign, 4 for malignant)\n",
    "\n",
    "The Cancer Observation csv file has the following format :\n",
    "\n",
    "    1000025,5,1,1,1,2,1,3,1,1,2\n",
    "    1002945,5,4,4,5,7,10,3,2,1,2\n",
    "    1015425,3,1,1,1,2,2,3,1,1,2\n",
    "    1016277,6,8,8,1,3,4,3,7,1,2\n",
    "    1017023,4,1,1,3,2,1,3,1,1,2\n",
    "\n",
    "In this scenario, we will build a logistic regression model to predict the label of malignant or not based on the following features:\n",
    "\n",
    "    Label → malignant or benign (1 or 0)\n",
    "    Features → columns 2-10 from the above list\n",
    "\n",
    "Let's read the downloaded file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data = sc.textFile(\"breast-cancer-wisconsin.data.txt\")\n",
    "\n",
    "print(\"File contains {} rows\".format(raw_data.count()))\n",
    "raw_data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and transform RDD to DataFrame (we define column names as follows):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_names = [\n",
    "    \"id\",            # Sample code number\n",
    "    \"thickness\",     # Clump Thickness\n",
    "    \"size\",          # Uniformity of Cell Size\n",
    "    \"shape\",         # Uniformity of Cell Shape\n",
    "    \"adhesion\",      # Marginal Adhesion\n",
    "    \"epithelial\",    # Single Epithelial Cell Size\n",
    "    \"nuclei\",        # Bare Nuclei\n",
    "    \"chromatin\",     # Bland Chromatin\n",
    "    \"nucleoli\",      # Normal Nucleoli\n",
    "    \"mitoses\",       # Mitoses\n",
    "    \"class\",         # Class\n",
    "]\n",
    "\n",
    "data = spark.createDataFrame(raw_data.map(lambda x: x.split(\",\")), col_names)\n",
    "print(\"DataFrame schema:\")\n",
    "data.printSchema()\n",
    "data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see all columns have string type although are numbers. Let's convert them to the respective type using `cast()` method. Also there is no need to use the `\"id\"` column for classification, that's why we will remove it.\n",
    "\n",
    "But here also the problem of presense of missing data takes place. If you attentively look at the dataset values, you may notice the `?` signs there. Let's check which columns have such values and how many, at first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "for col_name in col_names:\n",
    "    c = data.filter(data[col_name] == \"?\").count()\n",
    "    if c > 0:\n",
    "        print(\"The column '{}' has {} null values\".format(col_name, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we have about 2% of records with null values. We can remove them without loss of essential information and then convert all values to float type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Select rows without '?' in \"nuclei\" column\n",
    "data = data.filter(data[\"nuclei\"] != \"?\")\n",
    "\n",
    "# Remove \"id\" column\n",
    "data = data.drop(\"id\")\n",
    "del col_names[0]\n",
    "\n",
    "# Change type from string to double (is almost equivalent to float)\n",
    "for i in col_names:\n",
    "    data = data.withColumn(i, data[i].cast(DoubleType()))\n",
    "    \n",
    "print(\"DataFrame schema:\")\n",
    "data.printSchema()\n",
    "data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we noted that will use class labels 1 and 0 for malignant and benign (instead of 4 and 2). It is common used notations for binary classification. The [StringIndexer](https://spark.apache.org/docs/latest/ml-features.html#stringindexer) class allows doing this very easily. It encodes a string column of labels to a column of label indices and works not only with numerical values but also with strings that is very convinient.\n",
    "\n",
    "Let's look how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Create a StringIndexer and define two parameters\n",
    "# inputCol = name of the columns which values should be converted to label indices\n",
    "# outputCol = name of the columns containing these indices\n",
    "indexer = StringIndexer(\n",
    "    inputCol=\"class\", \n",
    "    outputCol=\"label\"\n",
    ")\n",
    "\n",
    "indexed_data = indexer.fit(data).transform(data)\n",
    "indexed_data.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, class `2.0` (benign) was replaced by `0.0` as well as class `4.0` (malignant) replaced by `1.0`.\n",
    "\n",
    "In order for the features to be used by a machine learning algorithm, the features are transformed and put into [Feature Vectors](https://spark.apache.org/docs/latest/mllib-data-types.html#local-vector), which are vectors of numbers representing the value for each feature.\n",
    "\n",
    "Below a [VectorAssembler](https://spark.apache.org/docs/latest/ml-features.html#vectorassembler) is used to transform and return a new DataFrame with all of the feature columns in a vector column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=col_names[:-1],   # all columns except for \"class\" and \"label\"\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "feature_data = assembler.transform(indexed_data)\n",
    "feature_data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the column `\"features\"` contains the list of all cell values in `\"thickness\"`, `\"size\"`, `\"shape\"`, `\"adhesion\"`, `\"epithelial\"`, `\"nuclei\"`,\t`\"chromatin\"`, `\"nucleoli\"` and\t`\"mitoses\"` columns with saving of the order\n",
    "\n",
    "<img src=\"images/vector_assembler.png\" width=90%>\n",
    "\n",
    "Now we can move to building classification model. This scheme demonstrates which steps should be implemented.\n",
    "\n",
    "<img src=\"images/classification_flow.png\">\n",
    "\n",
    "Thus, first of all we need to split all data into a training data set and a test data set using [`randomSplit()`](http://spark.apache.org/docs/2.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit) method, 70% of the data is used to train the model, and 30% will be used for testing (this percentages are defined as attributes of `randomSplit([train_ratio, test_ratio])`). But we need also only `\"labelIndex\"` and `\"features\"` columns for fit a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainingData, testData = feature_data.select(\"label\", \"features\").randomSplit([0.7, 0.3])\n",
    "print(\"trainingData has {} rows\".format(trainingData.count()))\n",
    "print(\"testData has {} rows\".format(testData.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train the [logistic regression model with elastic net regularization](https://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression).\n",
    "\n",
    "The model is trained by making associations between the input features and the labeled output associated with those features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create a LogisticRegression object \n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "# Fit the model\n",
    "lrModel = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next we use the test data to get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = lrModel.transform(testData) \n",
    "predictions.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As you can see, the previous model transform produced a new columns: `\"rawPrediction\"` (typically the direct probability/confidence calculation), `\"probablity\"` (probability of each class given the raw prediction,\n",
    "doing the computation in-place; in current example the first element in brackets correcponds to label `0` and the second to `1`; the largest wins) and `\"prediction\"` (predicted class).\n",
    "\n",
    "Let's calculate accuracy. It is the ratio of correctly classified object in test data to the total amount of object in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy = predictions.filter(predictions[\"prediction\"] == predictions[\"label\"]).count() / testData.count() * 100\n",
    "print(\"Accuracy = {0:.2f}%\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the predictions we may use a [`MulticlassClassificationEvaluator`](http://spark.apache.org/docs/2.0.0/api/python/_modules/pyspark/ml/evaluation.html) which returns a precision metric by comparing the test label column with the test prediction column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "accuracy2 = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = {0:.2f}%\".format(accuracy2 * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we achieve enough large accuracy on such small dataset without any advanced machine learning tricks and are able now classify patients automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Exercise 1\n",
    "> Convert **AUTHORIZATION_DATE** to Timestamp type. Then create culumn **TIME_DIFF** and find difference between **AUTHORIZATION_DATE** and **REQUEST_DATE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# type your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Exercise 2\n",
    "> Use the HiveContext group **amazon_df** by **TARGET_NAME** and find maximum, minimum and average values of **LOGIN**.\n",
    "> Don't forget register table in HiveContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# type your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Exercise 3\n",
    "> Calculate [precision and recall metrics](https://en.wikipedia.org/wiki/Precision_and_recall) for the provided classification example. Use both direct calculations and [`MulticlassClassificationEvaluator`](http://spark.apache.org/docs/2.0.0/api/python/_modules/pyspark/ml/evaluation.html). Try to interpret obtained results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# type your code here "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
