{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><center><h1>Elasticsearch</h1></center></u>\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/elastic_logo.png\" width=60%>\n",
    "\n",
    "[Elasticsearch](https://www.elastic.co/) is open-source full-text search and analytics engine based on [Lucene](https://en.wikipedia.org/wiki/Apache_Lucene). It was developed by Shay Banon and published in 2010. \n",
    "\n",
    "From the moment of its appearance on the market, the popularity and share of use are growing rapidly and today Elasticsearch is the leader among other search tools. Elasticsearch can be used for many purposes, particularly, financial services, government, healthcare, manufacturing, media & entertainment, software & technology, professional services, travel & transportation, telecommunications, retail. Many famous companies are alredy successfully use Elasticsearch, for example: DELL, eBay, Symantec, Netflix, Facebook, Cisco, Microsoft, Mozilla, Adobe, IBM, Docker, GitHub, SoundCloud and many-many other huge companies (more info you can find [here](https://www.elastic.co/use-cases)). \n",
    "\n",
    "The below graph shows the difference between [Apache Solr](http://lucene.apache.org/solr/) (this is another popular seach tool) and Elasticsearch by search interest.\n",
    "<img src=\"images/solr_vs_elasticsearch.jpg\" width=80%>\n",
    "\n",
    "With clear Elasticsearch you can install several packages:\n",
    "* [Kibana](https://www.elastic.co/downloads/kibana) - Kibana enables visual exploration and real-time analysis of your data in Elasticsearch. Kibana helps to understand large volumes of data.\n",
    "* [Filebeat](https://www.elastic.co/downloads/beats/filebeat) - is a log data shipper. Installed as an agent on your servers.\n",
    "* [Packetbeat](https://www.elastic.co/downloads/beats/packetbeat) - is a real-time network packet analyzer.\n",
    "* [Winlogbeat](https://www.elastic.co/downloads/beats/winlogbeat) - can capture event data from any event logs running on Windows system.\n",
    "* [Metricbeat](https://www.elastic.co/downloads/beats/metricbeat) - collect metrics from the operating system and from services running on the server. \n",
    "* [Heartbeat](https://www.elastic.co/downloads/beats/heartbeat) - tells you whether your services are reachable.\n",
    "* [Topbeat](https://www.elastic.co/downloads/beats/topbeat) - helps you monitor your servers by collecting metrics, for example: system-wide, per-process and file system statistics.\n",
    "* [Logstash](https://www.elastic.co/downloads/logstash) - need to ingest, transform, enrich, and output data.\n",
    "* [ES-Hadoop](https://www.elastic.co/downloads/hadoop) - allow you to use Elasticsearch from Hadoop environment.\n",
    "* [X-Pack](https://www.elastic.co/downloads/x-pack) - security, alerting, monitoring, reporting, and Graph in one pack. It comes with an interactive console called Sense, which makes it easy to talk to Elasticsearch directly from your browser.\n",
    "\n",
    "Before as you will know the basic concepts and how Elasticsearch works you need to know advantages and disadvantages.\n",
    "\n",
    "#### Advantages:\n",
    "* Elasticsearch is developed on Java, which makes it compatible on almost every platform.\n",
    "* Elasticsearch is real time, in other words after one second the added document is searchable in this engine.\n",
    "* Elasticsearch is distributed, which makes it easy to scale and integrate in any big organization.\n",
    "* Creating full backups are easy by using the concept of gateway, which is present in Elasticsearch.\n",
    "* Handling multi-tenancy is very easy in Elasticsearch when compared to Apache Solr.\n",
    "* Elasticsearch uses JSON objects as responses, which makes it possible to invoke the Elasticsearch server with a large number of different programming languages.\n",
    "* Elasticsearch supports almost every document type except those that do not support text rendering.\n",
    "\n",
    "#### Disadvantages:\n",
    "* Elasticsearch does not have multi-language support in terms of handling request and response data (only possible in JSON) unlike in Apache Solr, where it is possible in CSV, XML and JSON formats.\n",
    "* Elasticsearch also have a problem of Split brain situations, but in rare cases.\n",
    "* You can't write queries in SQL.\n",
    "* The distributed nature of Elasticsearch can have negative effects on data consistency.\n",
    "* Clear Elasticsearch does not have any built-in authentication or authorization system.\n",
    "\n",
    "#### How does it works?\n",
    "\n",
    "Using a restful API, Elasticsearch saves data and indexes it automatically. It assigns types to fields and that way a search can be done smartly and quickly using filters and different queries.\n",
    "\n",
    "It’s uses JVM in order to be as fast as possible. It distributes indexes in “shards” of data. It replicates shards in different nodes, so it’s distributed and clusters can function even if not all nodes are operational. Adding nodes is super easy and that’s what makes it so scalable.\n",
    "\n",
    "ES uses Lucene to solve searches. This is quite an advantage with comparing with, for example, Django query strings. A restful API call allows us to perform searches using json objects as parameters, making it much more flexible and giving each search parameter within the object a different weight, importance and or priority.\n",
    "\n",
    "The final result ranks objects that comply with the search query requirements. You could even use synonyms, autocompletes, spell suggestions and correct typos. While the usual query strings provides results that follow certain logic rules, ES queries give you a ranked list of results that may fall in different criteria and its order depend on how they comply with a certain rule or filter.\n",
    "\n",
    "ES can also provide answers for data analysis, like averages, how many unique terms and or statistics. This could be done using aggregations. To dig a little deeper in this feature check the documentation [here](https://www.elastic.co/guide/en/elasticsearch/reference/5.1/_basic_concepts.html).\n",
    "\n",
    "#### Basic concepts of Elasticsearch:\n",
    "* Cluster - it is a collection of one or more nodes (servers) that together holds your entire data and provides federated indexing and search capabilities across all nodes.\n",
    "* Node - it is part of your cluster, stores your data, and participates in the cluster’s indexing and search capabilities.\n",
    "* Index - it is a collection of different type of documents and document properties. In a single cluster, you can define as many indexes as you want. It is an analoques of a database.\n",
    "* Type - it is a collection of documents sharing a set of common fields present in the same index. Within an index, you can define one or more types. It is like a table in a database.\n",
    "* Document - it is a collection of fields that can be indexed and described in JSON format. It is similar to a row in a table.\n",
    "* Mapping - is the process of defining how a document and its fields are stored and indexed. This is analoques of a schema.\n",
    "* Shards - it is a fully-functional and independent \"index\" that can be hosted on any node in the cluster.\n",
    "* Replicas - it is a copies of index shards.\n",
    "\n",
    "#### Why Elasticsearch is fast? \n",
    "\n",
    "Your data in **`type`** will be split by default on 5 shards and when the request come in Elasticsearch coordinating node send the search request to all shards. Each shards return document IDs of 10 best matches. Then coordinating node find matches in returns of shards and return 10 best matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Installing and running Elasticsearch\n",
    "\n",
    "1. [Download](https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.2.2.zip) Elasticsearch and unzip files.\n",
    "\n",
    "2. [Download](https://artifacts.elastic.co/downloads/logstash/logstash-5.2.2.zip) Logstash and unzip files. Logstash need to ingest, transform, enrich, and output data. \n",
    "\n",
    "3. Install [python elasticsearch client](http://elasticsearch-py.readthedocs.io/en/master/index.html). This is a specific Python library for using Elasticsearch. Open Tetminal or Command Prompt and execute the following command\n",
    "\n",
    "    `pip install elasticsearch`\n",
    "\n",
    "Another way to work with Elasticsearch in Jupyter is the usage of the extension [ipython-elasticsearch](https://pypi.python.org/pypi/ipython-elasticsearch). Examples of using it you can see, for example, [here](https://github.com/graphaelli/ipython-elasticsearch/blob/master/Learn%20Elasticsearch%20with%20Jupyter.ipynb).\n",
    "\n",
    "Elasticsearch is now ready to run. You can start it up in the foreground with this, run this command in the Elasticsearch directory:\n",
    "\n",
    "    ./bin/elasticsearch\n",
    "    \n",
    "At first, let's import the **`python elasticsearch client`** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Simple way to check if Elasticsearch is working is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "es.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic CRUD, search, filtering, aggregation and mapping operations\n",
    "\n",
    "Now when Elasticsearch is running you need to insert data. \n",
    "\n",
    "We will use [these data](http://finance.yahoo.com/quote/AAPL/history?ltr=1) for learning Elasticsearch and trying all its features. Select \"Hostorical Data\" tab (1) and set \"Time Period\" from `03/01/2012` to `12/30/2016` (2), after that click on and \"Download Data\" button (3) (save image below) and save the file in CSV format anywhere (preferably to the folder where the current notebook lies). Let's call the file `\"table.csv\"`. \n",
    "\n",
    "<img src=\"images/download.jpg\" width=\"80%\">\n",
    "\n",
    "To use data from a CSV file you need to create **`logstash.conf`** file. Let's create it in **`/logstash-5.2.2/`**, but you can create it anywhere and then add path to them. Paste to the **`logstash.conf`** file the following code:\n",
    "<code> \n",
    "input {  \n",
    "  file {\n",
    "    path => \"/path/to/table.csv\"\n",
    "    start_position => \"beginning\"    \n",
    "  }\n",
    "}\n",
    "\n",
    "filter {  \n",
    "  csv {\n",
    "      separator => \",\"\n",
    "      columns => [\"Date\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\",\"Adj Close\"]\n",
    "  }\n",
    "  mutate {convert => {\"Date\" => \"string\"}}\n",
    "  mutate {convert => {\"Open\" => \"float\"}}\n",
    "  mutate {convert => {\"High\" => \"float\"}}\n",
    "  mutate {convert => {\"Low\" => \"float\"}}\n",
    "  mutate {convert => {\"Close\" => \"float\"}}\n",
    "  mutate {convert => {\"Volume\" => \"float\"}}\n",
    "  mutate {convert => {\"Adj Close\" => \"float\"}}\n",
    "}\n",
    "\n",
    "output {  \n",
    "    elasticsearch {\n",
    "        action => \"index\"\n",
    "        hosts => [\"127.0.0.1:9200\"]\n",
    "        index => \"finance_history\"\n",
    "        workers => 1\n",
    "    }\n",
    "    stdout {}\n",
    "}</code>\n",
    "\n",
    "**Note: Change `\"/path/to/table.csv\"` to the correct folder where you have saved the `\"table.csv\"` file.**\n",
    "\n",
    "> * In the input section we are telling Logstash to take the csv file as a datasource and start reading data at the beginning of the file.\n",
    "\n",
    "> * The filter section is used to tell Logstash in which data format our dataset is present (in this case csv). We give the names of the columns we want to keep in the output. Then converting all the fields containing numbers to float and `Date` to `string`.\n",
    "\n",
    "> * The output section is used to stream the input data to Elasticsearch. We specify the name of the index and hosts.\n",
    "\n",
    "You can read more about [input](https://www.elastic.co/guide/en/logstash/current/input-plugins.html), [filter](https://www.elastic.co/guide/en/logstash/current/filter-plugins.html) and [output](https://www.elastic.co/guide/en/logstash/current/output-plugins.html)\n",
    "\n",
    "Now you ready to load data in Elasticsearch. In the `Logstash` folder run this command:\n",
    "\n",
    "    ./bin/logstash -f /path/to/logstash.conf\n",
    "\n",
    "When data is loaded you can start working with it. Let's find the imported data for `\"table.csv\"` file. It can be done with the help of **`search`** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "es.search(index='finance_history', size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was returned the record which was saved to the Elasticsearch the first. This records may not coincide with the first record in the CSV file.\n",
    "\n",
    "We set **`index`** parameter to **`finance_history`** because this index we have set in the **`logstash.conf`** file.  The **`size`** parameter shows how many output results you will get, by default it is equal to 10.\n",
    "\n",
    "Now let's create and insert some additional data. From the previous output you can see that document have **`_type`** field. That means that all data are in **`logs`**. To add new data you need to use **`create`** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"Date\": \"2012-01-02\",\n",
    "    \"Open\": 408.24,\n",
    "    \"High\": 410.36,\n",
    "    \"Low\": 407.64,\n",
    "    \"Close\": 409.74,\n",
    "    \"Volume\": 67817400,\n",
    "    \"Adj Close\": 52.74\n",
    "}\n",
    "\n",
    "es.create(index='finance_history', doc_type='logs', id=1, body=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "* `doc_type` is the type of the document\n",
    "* `id` is document id\n",
    "* `body` is the document\n",
    "\n",
    "Now let's check whether just added data are in the index. For this you need to use **`get`** method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "es.get(index='finance_history', doc_type='logs', id=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can also update any field of any document using the **`update`** method. Let's update the value of the `\"High\"` field of the last added document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "  \"doc\": { \"High\": 411.12 }\n",
    "}\n",
    "\n",
    "es.update(index='finance_history', doc_type='logs', id=1, body=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update data you need to define in body `doc` or `script` key: `doc` simply updates a document, `script` allows to set your own exception. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "es.get(index='finance_history', doc_type='logs', id=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As you can see the `\"High\"` is changed and equals to `411.12`. Now let's remove this added document with the help of **`delete`** method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "es.delete(index='finance_history', doc_type='logs', id=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is successful deleted. If you try to find it you get response with [404 error](https://en.wikipedia.org/wiki/HTTP_404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "es.get(index='finance_history', doc_type='logs', id=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used previosly the **`search`** method, but now you meet with it more detailed. You can search by index, types and documents:\n",
    "\n",
    "* es.search() - Search across all indexes and all types.\n",
    "* es.search(index='finance_history') - Search across all types.\n",
    "* es.search(index='finance_history', doc_type='logs') - Search the documents in the `finance_history` index of type `logs`.\n",
    "\n",
    "Also it is possible to specify several `indexes` or `types`:\n",
    "    \n",
    "    es.search(index=['finance_history', 'movies_history'], doc_type=['logs', 'posts'])\n",
    "\n",
    "These are the most used parameters which can take the **`search`** method, all possible parameters you can find [here](https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.Elasticsearch.search):\n",
    "\n",
    "|Parameter|Description|\n",
    "|:---|------------|\n",
    "|`body`|The search definition using the [Query DSL](https://www.elastic.co/guide/en/elasticsearch/reference/5.2/query-dsl.html).|\n",
    "|`_source`|Set to false to disable retrieval of the _source field. You can also retrieve part of the document by using `_source_include` & `_source_exclude` (see the [request body](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-source-filtering.html) documentation for more details)|\n",
    "|`sort`|Sorting to perform. Can either be in the form of fieldName, or fieldName:asc/fieldName:desc. The fieldName can either be an actual field within the document, or the special _score name to indicate sorting based on scores. There can be several sort parameters (order is important).|\n",
    "|`size`|The number of hits to return. Defaults to 10.|\n",
    "\n",
    "ES has different type of search query:\n",
    "\n",
    "|Query|Description|\n",
    "|:---|------------|\n",
    "|[`match`](https://www.elastic.co/guide/en/elasticsearch/reference/5.2/query-dsl-match-query.html)|The standard query for performing full text queries, including fuzzy matching and phrase or proximity queries.|\n",
    "|[`match_phrase`](https://www.elastic.co/guide/en/elasticsearch/reference/5.2/query-dsl-match-query-phrase.html)|Like the match query but used for matching exact phrases or word proximity matches.|\n",
    "|[`match_phrase_prefix`](https://www.elastic.co/guide/en/elasticsearch/reference/5.2/query-dsl-match-query-phrase-prefix.html)|Like the match_phrase query, but does a wildcard search on the final word.|\n",
    "|[`multi_match`](https://www.elastic.co/guide/en/elasticsearch/reference/5.2/query-dsl-multi-match-query.html)|The multi-field version of the match query.|\n",
    "|[`common_terms`](https://www.elastic.co/guide/en/elasticsearch/reference/5.2/query-dsl-common-terms-query.html)|A more specialized query which gives more preference to uncommon words.|\n",
    "|[`query_string`](https://www.elastic.co/guide/en/elasticsearch/reference/5.2/query-dsl-query-string-query.html)|Supports the compact Lucene [query string syntax](https://www.elastic.co/guide/en/elasticsearch/reference/5.2/query-dsl-query-string-query.html#query-string-syntax), allowing you to specify `AND/OR/NOT` conditions and multi-field search within a single query string.|\n",
    "|[`simple_query_string`](https://www.elastic.co/guide/en/elasticsearch/reference/5.2/query-dsl-simple-query-string-query.html)|A simpler, more robust version of the query_string syntax suitable for exposing directly to users.|\n",
    "\n",
    "Let's find data where **`Date`** is **`1 February 2016`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = {\n",
    "    \"query\": {\n",
    "        \"query_string\" : {\n",
    "            \"default_field\" : \"Date\",\n",
    "            \"query\" : \"2016-02-01\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "es.search(index='finance_history', body=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "`default_field` is the field name where search will find the `query`. You can do the multi-field search by setting of the `fields` parameter. The folowing query will select only those documents where `\"Open\"` OR `\"High\"` fields are eqaul to `116.519997` OR `116.50`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = {\n",
    "    \"query\": {\n",
    "        \"query_string\" : {\n",
    "            \"fields\" : [\"Open\", \"High\"],\n",
    "            \"query\" : \"116.519997 OR 116.50\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "es.search(index='finance_history', body=query, sort='Date', _source=['Date', 'Open', 'High'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous request can be slightly confused, because it is not so obvious. Let's rewrite it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = {\n",
    "    \"query\": {\n",
    "        \"query_string\" : {\n",
    "            \"query\" : \"(Open:116.519997 OR High:116.50) OR (Open:116.50 OR High:116.519997)\"            \n",
    "        }\n",
    "    },\n",
    "    \"sort\": \"Date\"\n",
    "}\n",
    "es.search(index='finance_history', body=query, sort='Date', _source=['Date', 'Open', 'High'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You can use `filter` in query. `filter` is a part of `bool` `query`. `bool` `query` have the occurrence types:\n",
    "\n",
    "|Occurrence|Description|\n",
    "|:---|------------|\n",
    "|`must`|The clause (query) must appear in matching documents and will contribute to the score.|\n",
    "|`filter`|The clause (query) must appear in matching documents. However unlike must the score of the query will be ignored. Filter clauses are executed in [filter context](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-filter-context.html), meaning that scoring is ignored and clauses are considered for caching.|\n",
    "|`should`|The clause (query) should appear in the matching document. In a boolean query with no must or filter clauses, one or more should clauses must match a document. The minimum number of should clauses to match can be set using the [minimum_should_match](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-minimum-should-match.html) parameter.|\n",
    "|`must_not`|The clause (query) must not appear in the matching documents. Clauses are executed in [filter context](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-filter-context.html) meaning that scoring is ignored and clauses are considered for caching. Because scoring is ignored, a score of 0 for all documents is returned.|\n",
    "\n",
    "Let's filter the data and find those documents where `Date` contains 2015 year, `High` lies between 95 and 115 and show only one result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = {\n",
    "    \"query\": {\n",
    "        \"bool\" : {\n",
    "            \"filter\": [\n",
    "                { \"range\": { \"High\": { \"gte\": 95.00, \"lte\": 115.0} } },\n",
    "                { \"range\": { \"Date\": { \"gte\": \"2015-01-01\", \"lt\": \"2016-01-01\" }}}\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"size\": 1\n",
    "}\n",
    "es.search(index='finance_history', body=query, size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `range` query accepts the following parameters:\n",
    "\n",
    "|Parameter|Description|\n",
    "|:---|------------|\n",
    "|`gte`|Greater-than or equal to|\n",
    "|`gt`|Greater-than|\n",
    "|`lte`|Less-than or equal to|\n",
    "|`lt`|Less-than|\n",
    "|`boost`|Sets the boost value of the query, defaults to 1.0 |\n",
    "\n",
    "### Aggregation in Elasticsearch\n",
    "\n",
    "Now let's meet with aggregation. Aggregation helps to provide aggregated data based on a search query. It is based on simple building blocks called aggregations, that can be composed in order to build complex summaries of the data. The aggregation structure looks like:\n",
    "<code>\n",
    "\"aggregations\" : {\n",
    "    \"&lt;aggregation_name&gt;\" : {\n",
    "        \"&lt;aggregation_type&gt;\" : {\n",
    "            &lt;aggregation_body&gt;\n",
    "        }\n",
    "        [,\"meta\" : {  [&lt;meta_data_body&gt;] } ]?\n",
    "        [,\"aggregations\" : { [&lt;sub_aggregation&gt;]+ } ]?\n",
    "    }\n",
    "    [,\"&lt;aggregation_name_2&gt;\" : { ... } ]*\n",
    "}</code>\n",
    "\n",
    "Aggregation in ElasticSearch can be of four types:\n",
    "\n",
    "|Type|Description|\n",
    "|:---|------------|\n",
    "|[`Metric`](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics.html)|Aggregations that keep track and compute metrics over a set of documents.|\n",
    "|[`Bucket`](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket.html)|Build buckets, where each bucket is associated with a key and a document criterion. When the aggregation is executed, all the buckets criteria are evaluated on every document in the context and when a criterion matches, the document is considered to \"fall in\" the relevant bucket. By the end of the aggregation process, we’ll end up with a list of buckets - each one with a set of documents that \"belong\" to it.|\n",
    "|[`Pipeline`](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-pipeline.html)|Aggregations that aggregate the output of other aggregations and their associated metrics.|\n",
    "|[`Matrix`](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-matrix.html)|Operate on multiple fields and produce a matrix result based on the values extracted from the requested document fields. Unlike metric and bucket aggregations, this aggregation family does not yet support scripting.|\n",
    "\n",
    "Using the `Metric` aggregation find average, maximum, minimum, count and summary of `\"Close\"` values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = {\n",
    "    \"size\": 0,\n",
    "    \"aggs\": {\n",
    "        \"avg_close\" : { \"avg\" : { \"field\" : \"Close\" } },\n",
    "        \"max_close\" : { \"max\" : { \"field\" : \"Close\" } },\n",
    "        \"min_close\" : { \"min\" : { \"field\" : \"Close\" } },\n",
    "        \"total_close\" : { \"sum\" : { \"field\" : \"Close\" } },\n",
    "        \"count_close\" : { \"value_count\" : { \"field\" : \"Close\" } }\n",
    "    }\n",
    "}\n",
    "\n",
    "es.search(index='finance_history', body=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But in `Metric` aggregation multi-value aggregation exists and the previous query can be replaced by `extended_stats` or `stats`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = {\n",
    "    \"size\": 0,\n",
    "    \"aggs\": {\n",
    "        \"stats_close\" : { \"stats\" : { \"field\" : \"Close\" } },\n",
    "        \"extended_stats_close\" : { \"extended_stats\" : { \"field\" : \"Close\" } }\n",
    "    }\n",
    "}\n",
    "\n",
    "es.search(index='finance_history', body=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, you need to find the average of difference between `Close` and `Low` values. In ElasticSearch you can write your own script of aggregation just need to use `scripted_metric` aggregation. The `scripted_metric` aggregation contain four executions:\n",
    "\n",
    "|Execution|Description|\n",
    "|:---|------------|\n",
    "|`init_script`|Allows to set up any initial state.|\n",
    "|`map_script`|Executed once per document collected. This is the only required script. If no **`combine_script`** is specified, the resulting state needs to be stored in an object named **_agg**.|\n",
    "|`combine_script`|Executed once on each shard after document collection is complete. Allows the aggregation to consolidate the state returned from each shard. If a **`combine_script`** is not provided the combine phase will return the aggregation variable. |\n",
    "|`reduce_script `|Executed once on the coordinating node after all shards have returned their results. The script is provided with access to a variable **`_aggs`** which is an array of the result of the **`combine_script`** on each shard. If a **`reduce_script`** is not provided the reduce phase will return the **`_aggs`** variable. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = {\n",
    "    \"size\": 0,\n",
    "    \"aggs\": {\n",
    "        \"average_of_diff\" : { \n",
    "            \"scripted_metric\" : {\n",
    "                \"init_script\" : \"params._agg.transactions = []\",\n",
    "                \"map_script\" : \"params._agg.transactions.add(doc.Close.value - doc.Low.value)\", \n",
    "                \"combine_script\" : \"float diff = 0; int count = 0; for (v in params._agg.transactions) { diff += v; count += 1 } return [diff, count]\",\n",
    "                \"reduce_script\" : \"float diff = 0; int count = 0; for (v in params._aggs) { diff += v[0]; count += v[1] } return diff / count\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "es.search(index='finance_history', body=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the **`init_script`** we define the initial state. Imagine that in python it will be empty list **`[]`**.\n",
    "* In the **`map_script`** we find difference between **`Close`** and **`Low`** values and add it in list.\n",
    "* In the **`combine_script`** we define two varieble **`diff`** and **`count`**. Then we itterate throw all found differences, find summary of differences and count the number of it. Then return found summary and count of each shards(default shards is 5).\n",
    "* In the **`reduce_script`** we find total summary of differences and count. Then return the average value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping in Elasticsearch\n",
    "\n",
    "Mapping is the process of defining how a document and its fields are stored and indexed.\n",
    "\n",
    "To be able to treat date fields as dates, numeric fields as numbers, and string fields as full-text or exact value strings, Elasticsearch needs to know what type of data each field contains. This information is included in the mapping. We can use mappings to define:\n",
    "\n",
    "* which string fields should be treated as full text fields.\n",
    "* which fields contain numbers, dates, or geolocations.\n",
    "* whether the values of all fields in the document should be indexed into the catch-all [_all](https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-all-field.html) field.\n",
    "* the [format](https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-date-format.html) of date values.\n",
    "\n",
    "What you should know about mapping:\n",
    "* existing type and field mappings cannot be updated, because changing the mapping would mean invalidating already indexed documents. So, to avoid this situation, you should create a new index with the correct mappings and reindex your data into that index.\n",
    "* fields and mapping types don’t need to be defined before using, because Elasticsearch has the [dynamic mapping](https://www.elastic.co/guide/en/elasticsearch/reference/current/dynamic-mapping.html), new mapping types, and new field names will be added automatically, just by indexing a document. \n",
    "\n",
    "To see already exist mapping you need use **`indices`** attribute and **`get_mapping`** method. Elasticsearch instance has attributes cat, cluster, indices, ingest, nodes, snapshot and tasks that provide access to instances of [CatClient](https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.CatClient), [ClusterClient](https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.ClusterClient), [IndicesClient](https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient), [IngestClient](https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IngestClient), [NodesClient](https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.NodesClient), [SnapshotClient](https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.SnapshotClient) and [TasksClient](https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.TasksClient) respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "es.indices.get_mapping(index='finance_history')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create indices with defined types of fields in documents with the help of mapping. Let's create new index with mapping. For example, we create an index `movies` of type `movie` that will contain such data about movies: `\"title\"` (movie's title), `\"director\"`, `\"release date\"` and `\"rating\"` with specific types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = {\n",
    "    \"mappings\": {\n",
    "        \"movie\" : {\n",
    "            \"properties\" : {\n",
    "                \"title\": {\"type\": \"string\"},\n",
    "                \"director\": {\"type\": \"string\"},\n",
    "                \"release date:\": {\"type\": \"date\", \"format\": \"yyyy-MM-dd\"},\n",
    "                \"rating\": {\"type\": \"float\"},\n",
    "            }           \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "es.indices.create(index='movies', body=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert a new document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"title\": \"Forrest Gump\",\n",
    "    \"director\": \" Robert Zemeckis\",\n",
    "    \"release date:\": \"1994-07-01\",\n",
    "    \"rating\": 9\n",
    "}\n",
    "# Create\n",
    "es.create(index='movies', doc_type='movie', id=1, body=data)\n",
    "# and check at once\n",
    "es.get(index='movies', doc_type='movie', id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "es.indices.get_mapping(index='movies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite of we write `integer` value into `\"rating\"` field it will be processed as `float` at aggregation, for example.  To delete index you need the **`delete`** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "es.indices.delete(index='movies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full-text search in Elasticsearch\n",
    "\n",
    "Let’s show the difference between speeds of work of pure Python search and Elasticsearch. For this example you need to  download the [Gutenberg dataset](https://docs.google.com/uc?id=0B2Mzhc7popBga2RkcWZNcjlRTGM&export=download) and extract it. After that create mapping for new index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = {\n",
    "    \"mappings\": {\n",
    "        \"book\" : {\n",
    "            \"properties\" : {\n",
    "                \"author\": {\"type\": \"string\"},\n",
    "                \"title\": {\"type\": \"string\"},\n",
    "                \"story:\": {\"type\": \"string\"}\n",
    "            }           \n",
    "        }\n",
    "    }\n",
    "}\n",
    "es.indices.create(index='books', body=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Get all names from Gutenberg dataset. If you download the dataset in folder with notebook, don't edit the path just choose your system (below we provide commands for both Linux and Windows operating systems), if not please add the path to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you use Linux\n",
    "book_names = !ls Gutenberg/txt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you use Windows\n",
    "book_names = !dir Gutenberg/txt/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Add dataset in Elasticsearch. It takes around 15 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import time to see difference\n",
    "from time import time\n",
    "\n",
    "# Please set the correct path to the dataset \n",
    "path = 'Gutenberg/txt/'\n",
    "\n",
    "t0 = time()\n",
    "for id_book, book in enumerate(book_names):\n",
    "    # Open the downloaded dataset to understand why we process it in such way\n",
    "    book_s = book.split('___')\n",
    "    author, title = book_s[0], book_s[1][:-4].replace('\"', '')\n",
    "    \n",
    "    with open(path + book, encoding='iso-8859-15') as f:\n",
    "        story = \" \".join([i for i in f.read().splitlines() if i != ''])\n",
    "\n",
    "    doc = {\n",
    "        \"author\": author,\n",
    "        \"title\": title,\n",
    "        \"story\": story\n",
    "    }\n",
    "    es.create(index='books', doc_type='book', id=id_book, body=doc)\n",
    "es_add_t = time() - t0\n",
    "print(\"Dataset added to Elasticsearch in {} seconds\".format(es_add_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Find how match books are in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "es.search(index='books', size=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we have 3036 books in Elasticsearch. Let's take rows from the middle book, which is `John Dryden___Discourses on Satire and Epic Poetry.txt`. We choose this phrase **`Pulverulenta putrem sonitu quatit ungula campum.`** for its finding in all books with the help of Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"match_phrase\" : {\n",
    "            \"story\" : \"Pulverulenta putrem sonitu quatit ungula campum.\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "print(es.search(index='books', body=query, size=5, _source=['author', 'title']))\n",
    "es_search_t = time() - t0\n",
    "print(\"Elasticsearch find matches in {} seconds\".format(es_search_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find this row with pure Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "def python_search(s):\n",
    "    python_data = []\n",
    "    for id_book, book in enumerate(book_names):\n",
    "        book_s = book.split('___')\n",
    "        author, title = book_s[0], book_s[1][:-4].replace('\"', '')\n",
    "\n",
    "        with open(path + book, encoding='iso-8859-15') as f:\n",
    "            story = \" \".join([i for i in f.read().splitlines() if i != ''])\n",
    "        \n",
    "        if s in story:\n",
    "            doc = {\n",
    "                \"author\": author,\n",
    "                \"title\": title,\n",
    "                \"story\": story\n",
    "            }\n",
    "            python_data.append(doc)\n",
    "    return python_data\n",
    "\n",
    "s = \"Pulverulenta putrem sonitu quatit ungula campum.\"\n",
    "matches = python_search(s)\n",
    "python_search_t = time() - t0\n",
    "print(\"Python find matches in {} seconds\".format(python_search_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see Elasticsearch worked much more faster than Python.\n",
    "\n",
    "### Visualization using Kibana\n",
    "\n",
    "Elasticsearch outputs can be visualize with the help of [Kibana](https://www.elastic.co/products/kibana) data visualization plugin for Elasticsearch. Kibana enables visual exploration and real-time analysis of your data in Elasticsearch. \n",
    "\n",
    "At first you need to install Kibana. Go to this [page](https://www.elastic.co/downloads/kibana) and download Kibana to the folder where you saved Elasticsearch and Logstash. \n",
    "\n",
    "> Note, Kibana and Elasticsearhc must be of the same version!\n",
    "\n",
    "To run Kibana, execute this command in the Kibana directory:\n",
    "\n",
    "    ./bin/kibana\n",
    "\n",
    "When Kibana is running you can open the url http://localhost:5601. \n",
    "\n",
    "On start Kibana will propose you to configurate an index. You need to set index to **`finance_history`** and disable **` Index contains time-based events `**, then click on the `Create` button\n",
    "\n",
    "<img src=\"images/k01.png\">\n",
    "\n",
    "Now go to the **`Visualize`** tab and create a new **`Area chart`**.\n",
    "\n",
    "<img src=\"images/k02.png\">\n",
    "\n",
    "When you choose area chart, you need to set index to the value **`finance_history`** (it is index we are currently using). Now you can build area chart. On Y-Axis we choose **`Average`** of **`Close`** values.\n",
    "\n",
    "<img src=\"images/k03.png\">\n",
    "\n",
    "In the **`buckets`** area choose X-Axis and set **`Aggregation`** to **`Date Histogram`**, **`Field`** to **`Date`** and **`Interval`** to **`Monthly`**.\n",
    "\n",
    "<img src=\"images/k04.png\">\n",
    "\n",
    "In the right corner click on the **`Save`** button and save the area chart. Then go back to **`Visualize`** and create new **`Vertical bar chart`**. \n",
    "\n",
    "On Y-Axis we choose **`Median`** of **`Volume`** values. On the X-Axis set field like in **`Area chart`** except for **`Interval`**, **`Interval`** set to **`Weekly`**. Also in the top right corner you can shoose the colors of bars. Finaly save it.\n",
    "\n",
    "<img src=\"images/k05.png\">\n",
    "\n",
    "Next create the **`Metric`** and save it.\n",
    "\n",
    "<img src=\"images/k06.png\">\n",
    "\n",
    "Then go to the **`Dashboard`** tab and add saved charts and metrics.\n",
    "\n",
    "<img src=\"images/k07.png\">\n",
    "\n",
    "You can also save the dashboard. \n",
    "\n",
    "As you can see visualization with Kibana it is very easy. Visualization helps you to more cleary understand the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
